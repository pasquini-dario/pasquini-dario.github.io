<!DOCTYPE HTML>

<html>
	<head>
		<title>Dario's personal page</title>
		<meta name="google-site-verification" content="4ufx4V1x6lKJzuGnX-YoqdGxtML7p5mhmRjg6EPlgvA" />
		<meta name="description" content="Dario Pasquini personal page">
		<meta http-equiv="content-type" content="text/html;charset=UTF-8">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->

			<header id="header">
				<div class="inner">
					<a href="#" class="image avatar" id="avatar"><img src="me.png" alt="" /></a>
					<p id="myname"> Dario Pasquini 🇮🇹</p>
					<a id="side_a" href="mailto:dario.pasquini@epfl.ch">✉️ dario.pasquini@epfl.ch</a>
					<br>
					<br>
					<a id="side_a" href="https://github.com/pasquini-dario/">🐱 GitHub</a>
					<br>
					<a id="side_a" href="https://scholar.google.com/citations?user=L38xA_8AAAAJ&hl">🎓 Scholar</a>
					<br>
					<a id="side_a" href="../CV.pdf">🗒️ Ext. C.V.</a>
				</div>
			</header>

		<!-- Main -->
			<div id="main">


				<h2>Status:</h2>
				<ul id="no-bullets">
			<li>
				Postdoc at EPFL (<a href="https://spring.epfl.ch">Spring lab</a> 🇨🇭); <i>"Semi-honest model unbeliever"</i>  seeking security and privacy measures that transcend trust, ideal assumptions, and heuristic security definitions <i id="lighter"> (but, in actuality, I spend most of my time either breaking ML models or building ML models to break stuff)</i>.
			</li>
			<li>
				Working on:
					<ul id="workingon">
						<li>Security and Privacy in Machine Learning:</li>
						<ul id="multiple">
							<li> Collaborative Learning. </li>
							<li> LLMs <i id="lighter">(yes, I know..)</i>.</li>
						</ul>
						<li>Password Security.</li>
						<li>Crypto systems Security and Privacy <i id="lighter">(when that happens)</i>.</li>
					</ul>
				</li>
			</ul>

			<br>

		<h2>News:</h2>
		<ul>
				<li> 👔 <b>On the job market.</b> 👔</li>
				<li> <b>[Coming soon] </b> <i>"Exposing and Addressing the Leakage in Second Generation Compromised Credential Checking Services"</i>
					</li>
		</ul>

		<br>

			<h2>Works I like to talk about:</h2>
			<ul>
				<li  id="mainpub"> <b>[IEEE S&P'24] </b>- <b>Dario Pasquini</b>,  Giuseppe Ateniese, Carmela Troncoso.<br>
					<a href="https://arxiv.org/abs/2301.07628">Universal Neural-Cracking-Machines: Self-Configurable Password Models from Auxiliary Data</a>.
						<ul id="multiple">
						<li>
							[Derived talk] - <i> <a href="https://passwordscon.org/wp-content/uploads/2023/05/Dario-Pasquini.pdf">"Password Alchemy with Universal Neural-Cracking-Machines: Transmuting email addresses into Password Models"</a> </i> at <a href="https://passwordscon.org/passwordscon-2023-bergen/">PasswordsCon'23 (Bergen)</a>.
						</li>
						</ul>
				</li>
				<li  id="mainpub"> <b>[IEEE S&P'23]</b> - <b>Dario Pasquini</b>, Mathilde Raynal, Carmela Troncoso.<br>
					<a href="https://arxiv.org/pdf/2205.08443">On the (In)security of Peer-to-Peer Decentralized Machine Learning</a>.
				</li>
				<li  id="mainpub"> <b>[ACM CCS'22]</b> - <b>Dario Pasquini</b>, Danilo Francati, Giuseppe Ateniese.<br>
					<a href="https://arxiv.org/pdf/2111.07380">Eluding Secure Aggregation in Federated Learning via Model Inconsistency</a>.
				</li>
				<li  id="mainpub">
					<b>[ACM CCS'21]</b> - <b>Dario Pasquini</b>, Giuseppe Ateniese, Massimo Bernaschi.<br>
				<a href="https://arxiv.org/abs/2012.02670">Unleashing the Tiger: Inference Attacks on Split Learning</a>.
				</li>
				<li  id="mainpub">
					<b>[USENIX Sec'21]</b> - <b>Dario Pasquini</b>, Marco Cianfriglia, Giuseppe Ateniese, Massimo Bernaschi.<br>
					<a href="https://arxiv.org/abs/2010.12269">Reducing Bias in Modeling Real-world Password Strength via Deep Learning and Dynamic Dictionaries</a>.
				</li>
				<li  id="mainpub">
					<b>[IEEE S&P'21]</b> - <b>Dario Pasquini</b>, Ankit Gangwal, Giuseppe Ateniese, Massimo Bernaschi, Mauro Conti.<br>
					<a href="https://arxiv.org/abs/1910.04232">Improving Password Guessing via Representation Learning</a>.
	<!-- 			<li id="bib">
					<b>[ESORICS 20]</b> - <b>Dario Pasquini</b>, Giuseppe Ateniese, Massimo Bernaschi.
					<a href="https://arxiv.org/abs/2004.07179">Interpretable probabilistic password strength meters via deep learning</a>.
				</li> -->
			</ul>

<br>

  <h3> Program Committees: </h3>
	<ul id="no-bullets">
		<li>
			<a href="https://www.sigsac.org/ccs/CCS2023/">ACM CCS 2023</a>, <a href="https://www.usenix.org/conference/usenixsecurity23">USENIX Sec 2023</a>, <a href="https://satml.org">IEEE SaTML 2024</a>.
		</li>
	</ul>



	<!--
			<h3>Personal projects:</h3>
			<ul id="no-bullets">
				<li>
					<a href="https://pasquini-dario.github.io/DeepPasswd/">DeepPasswd</a> (just a POC)
				</li>
				<li>
					<strike><a href="https://github.com/TheAdamProject">Adam: The First Cracker</a></strike> (one day...)
				</li>
			</ul>


			<h3>Media coverage:</h3>
			<ul>
				<li>
					<a href="https://edtechmagazine.com/higher/article/2020/10/3-ways-ai-can-help-users-avoid-weak-passwords">EdTech: 3 Ways That AI Can Help Users Avoid Weak Passwords</a>
				</li>
				<li>
					<a href="https://www.stevens.edu/news/new-tool-helps-public-test-their-own-passwords--and-bolster-security"> Stevens News: New Tool Helps Public Test Their Own Passwords — and Bolster Security </a>
				</li>
			</ul>

 -->

	</div>

	</body>
</html>
